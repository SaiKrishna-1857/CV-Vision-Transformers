{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36027d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vision Transformer (ViT) in PyTorch\n",
    "\n",
    "A PyTorch implement of Vision Transformers as described in:\n",
    "\n",
    "'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n",
    "    - https://arxiv.org/abs/2010.11929\n",
    "\n",
    "`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n",
    "    - https://arxiv.org/abs/2106.10270\n",
    "\n",
    "The official jax code is released and available at https://github.com/google-research/vision_transformer\n",
    "\n",
    "Acknowledgments:\n",
    "* The paper authors for releasing code and weights, thanks!\n",
    "* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\n",
    "for some einops/einsum fun\n",
    "* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n",
    "* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n",
    "\n",
    "Hacked together by / Copyright 2020, Ross Wightman\n",
    "\"\"\"\n",
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from helpers_vit import PatchEmbed, Mlp, DropPath, trunc_normal_, lecun_normal_, named_apply\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainableEltwiseLayer(nn.Module):\n",
    "    def __init__(self, n, h, w):\n",
    "        super(TrainableEltwiseLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.Tensor(1, n, h, w))  # define the trainable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assuming x is of size b-1-h-w\n",
    "        return x * self.weights  # element-wise multiplication\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "#         #PEFT\n",
    "#         self.lk = nn.Linear(head_dim,head_dim,bias=False)\n",
    "#         nn.init.ones_(self.lk.weight)\n",
    "#         self.lv = nn.Linear(head_dim,head_dim,bias=False)\n",
    "#         nn.init.ones_(self.lv.weight)\n",
    "        \n",
    "        #PEFT\n",
    "        self.lk = TrainableEltwiseLayer(self.num_heads, 197, head_dim)\n",
    "        nn.init.ones_(self.lk.weights)\n",
    "        self.lv = TrainableEltwiseLayer(self.num_heads, 197, head_dim)\n",
    "        nn.init.ones_(self.lv.weights)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "        \n",
    "        \n",
    "        k = self.lk(k)\n",
    "        v = self.lv(v)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResPostBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_values = init_values\n",
    "\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # NOTE this init overrides that base model init with specific changes for the block type\n",
    "        if self.init_values is not None:\n",
    "            nn.init.constant_(self.norm1.weight, self.init_values)\n",
    "            nn.init.constant_(self.norm2.weight, self.init_values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.norm1(self.attn(x)))\n",
    "        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            num_parallel=2,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            init_values=None,\n",
    "            drop=0.,\n",
    "            attn_drop=0.,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_parallel = num_parallel\n",
    "        self.attns = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        for _ in range(num_parallel):\n",
    "            self.attns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('attn', Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "            self.ffns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('mlp', Mlp(dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "\n",
    "    def _forward_jit(self, x):\n",
    "        x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)\n",
    "        x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)\n",
    "        return x\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def _forward(self, x):\n",
    "        x = x + sum(attn(x) for attn in self.attns)\n",
    "        x = x + sum(ffn(x) for ffn in self.ffns)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.jit.is_scripting() or torch.jit.is_tracing():\n",
    "            return self._forward_jit(x)\n",
    "        else:\n",
    "            return self._forward(x)\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            num_classes=1000,\n",
    "            global_pool='token',\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            num_heads=12,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            init_values=None,\n",
    "            class_token=True,\n",
    "            no_embed_class=False,\n",
    "            pre_norm=False,\n",
    "            fc_norm=None,\n",
    "            drop_rate=0.,\n",
    "            attn_drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            weight_init='',\n",
    "            embed_layer=PatchEmbed,\n",
    "            norm_layer=None,\n",
    "            act_layer=None,\n",
    "            block_fn=Block,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            global_pool (str): type of global pooling for final sequence (default: 'token')\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            init_values: (float): layer-scale init values\n",
    "            class_token (bool): use class token\n",
    "            fc_norm (Optional[bool]): pre-fc norm after pool, set if global_pool == 'avg' if None (default: None)\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            weight_init (str): weight init scheme\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            act_layer: (nn.Module): MLP activation layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert global_pool in ('', 'avg', 'token')\n",
    "        assert class_token or global_pool != 'token'\n",
    "        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.global_pool = global_pool\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_prefix_tokens = 1 if class_token else 0\n",
    "        self.no_embed_class = no_embed_class\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n",
    "        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            block_fn(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                init_values=init_values,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n",
    "\n",
    "        # Classifier Head\n",
    "        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        if weight_init != 'skip':\n",
    "            self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'moco', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.cls_token is not None:\n",
    "            nn.init.normal_(self.cls_token, std=1e-6)\n",
    "        #named_apply(get_init_weights_vit(mode, head_bias), self)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        init_weights_vit_timm(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def group_matcher(self, coarse=False):\n",
    "        return dict(\n",
    "            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n",
    "            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n",
    "        )\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        self.grad_checkpointing = enable\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes: int, global_pool=None):\n",
    "        self.num_classes = num_classes\n",
    "        if global_pool is not None:\n",
    "            assert global_pool in ('', 'avg', 'token')\n",
    "            self.global_pool = global_pool\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def _pos_embed(self, x):\n",
    "        if self.no_embed_class:\n",
    "            # deit-3, updated JAX (big vision)\n",
    "            # position embedding does not overlap with class token, add then concat\n",
    "            x = x + self.pos_embed\n",
    "            if self.cls_token is not None:\n",
    "                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        else:\n",
    "            # original timm, JAX, and deit vit impl\n",
    "            # pos_embed has entry for class token, concat then add\n",
    "            if self.cls_token is not None:\n",
    "                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "            x = x + self.pos_embed\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self._pos_embed(x)\n",
    "        x = self.norm_pre(x)\n",
    "        if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "            x = checkpoint_seq(self.blocks, x)\n",
    "        else:\n",
    "            x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        if self.global_pool:\n",
    "            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n",
    "        x = self.fc_norm(x)\n",
    "        return x if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2894b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37dfd566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (lk): TrainableEltwiseLayer()\n",
       "        (lv): TrainableEltwiseLayer()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d5bb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_img = torch.rand(1, 3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97e63580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7930e-01, -4.5656e-01, -5.9838e-02, -7.7341e-01,  1.1835e-01,\n",
       "          8.2139e-01,  3.0746e-01, -2.0483e-01, -8.4935e-02,  5.0284e-01,\n",
       "         -4.2860e-01, -9.3118e-01,  3.2781e-01, -8.0264e-01,  4.0158e-01,\n",
       "          3.7164e-01,  5.0826e-01,  2.3981e-01,  1.6106e-01, -1.9247e-01,\n",
       "          1.0259e+00, -6.1552e-02, -6.3823e-01, -8.9428e-02,  3.0333e-01,\n",
       "         -2.7180e-01,  3.3407e-01,  7.0951e-01, -1.0148e+00,  6.8399e-02,\n",
       "          2.4802e-01,  8.5977e-02, -4.3385e-01, -6.6915e-01, -7.6593e-01,\n",
       "         -1.5652e-01,  1.7460e-01,  1.0372e+00, -2.8063e-01, -2.7648e-01,\n",
       "         -9.1799e-01,  5.0637e-02, -5.7462e-01,  2.0365e-01,  5.5312e-01,\n",
       "          3.4994e-01,  7.1423e-01,  6.9936e-01,  2.0209e-01, -4.2537e-01,\n",
       "         -2.6587e-01,  9.8400e-02, -8.5219e-01,  1.0237e+00, -4.3022e-01,\n",
       "          5.7497e-02,  4.7151e-01, -4.2180e-01,  8.3359e-01, -8.6090e-01,\n",
       "          1.4519e-01,  6.6140e-01, -2.8986e-01, -4.5380e-01, -1.3201e-01,\n",
       "          3.7189e-01, -7.2793e-01, -7.1931e-01, -4.0621e-01, -3.3448e-02,\n",
       "          3.8601e-02, -2.4914e-01,  3.2237e-01,  7.2017e-01,  7.6786e-01,\n",
       "         -3.7407e-01, -7.6367e-02,  1.2826e-01, -1.6512e+00,  5.3812e-01,\n",
       "         -2.7365e-01, -5.0820e-01, -1.4003e+00,  9.9897e-01, -5.3478e-02,\n",
       "         -1.5213e-01,  1.0413e-01,  1.0712e+00,  1.0197e+00, -5.9220e-01,\n",
       "         -3.8447e-02,  8.7044e-02, -9.9316e-01, -1.1245e+00,  1.3000e-01,\n",
       "          6.4005e-01, -2.0206e-01,  9.0075e-01, -2.6105e-01, -1.2410e+00,\n",
       "          1.5147e-02, -3.7776e-01, -1.8360e-02,  2.5999e-01,  6.9478e-01,\n",
       "          4.6820e-02, -1.5480e-01,  6.2950e-02,  2.7600e-01,  2.2290e-01,\n",
       "          3.9291e-01, -4.1602e-01,  3.8280e-01, -3.1283e-02, -8.9375e-01,\n",
       "         -7.5336e-01,  3.4831e-01, -2.5584e-01, -2.2616e-01, -4.5230e-01,\n",
       "         -3.9877e-01,  5.4228e-01, -4.0548e-01, -1.9778e-01,  1.3228e+00,\n",
       "          1.1999e+00, -6.1440e-01, -5.5791e-01, -5.9973e-01, -6.1106e-02,\n",
       "         -1.0434e+00, -1.0804e-01, -2.6589e-01,  1.5160e-01,  1.4858e-01,\n",
       "         -9.9258e-01, -3.0548e-01,  2.6357e-01,  2.7814e-01,  1.3913e+00,\n",
       "         -2.8958e-01, -2.1324e-01, -8.3397e-01,  3.1355e-01, -4.7637e-01,\n",
       "         -6.0012e-01, -7.7997e-01,  1.0419e-01,  2.0165e-01, -4.4230e-01,\n",
       "          9.9901e-01, -3.9510e-01, -5.1969e-01,  3.7569e-01, -1.5665e-01,\n",
       "          1.2977e-01, -7.8255e-01, -4.4859e-03,  4.6079e-01,  2.5547e-01,\n",
       "          1.3779e+00, -4.2092e-04, -1.3891e+00,  5.3715e-01, -1.2489e-01,\n",
       "         -3.9820e-01, -2.7148e-01, -5.8310e-01, -9.6544e-02, -7.2823e-01,\n",
       "          3.9041e-01,  2.1760e-01,  1.1580e-01,  3.3178e-01, -1.4708e-01,\n",
       "          2.1074e-01,  5.2754e-01,  1.0093e+00,  9.1825e-02,  6.3954e-01,\n",
       "          3.3742e-01,  8.3075e-01,  1.1076e-01,  2.0252e-01, -5.6273e-01,\n",
       "         -2.8413e-02,  3.1631e-01, -2.0282e-01,  5.4051e-01, -4.2278e-01,\n",
       "         -4.1050e-01, -8.1408e-01, -5.3520e-01, -5.0095e-01,  3.0721e-01,\n",
       "         -3.8228e-01, -7.0131e-02, -6.5779e-01,  1.7060e+00, -2.1913e-03,\n",
       "         -3.3264e-01, -3.9959e-01, -8.6021e-01,  1.4073e+00, -4.5773e-01,\n",
       "         -6.2230e-02, -3.7811e-01, -1.8899e-01,  7.3496e-01, -1.4720e-01,\n",
       "         -3.0335e-01,  5.6727e-01, -8.9036e-02, -7.6266e-02, -9.4281e-01,\n",
       "          4.8329e-01, -1.1525e+00, -1.3742e-01,  3.1998e-01, -2.9930e-01,\n",
       "         -5.3898e-02,  1.1886e-01,  5.4342e-01, -2.9317e-01, -7.2761e-01,\n",
       "          3.8722e-01,  1.6410e-01,  8.1222e-02, -7.2918e-01,  1.9979e-01,\n",
       "          7.7342e-01, -4.5903e-01, -8.4841e-02,  1.3526e-01, -1.9616e-01,\n",
       "          4.3399e-01,  6.3150e-02, -7.8287e-01, -3.0968e-02, -9.6556e-01,\n",
       "          9.7155e-01, -5.9350e-01,  4.3049e-01,  6.5935e-01, -3.7012e-01,\n",
       "         -3.5938e-01,  4.8255e-01,  1.0453e+00, -1.1961e-01, -4.6973e-01,\n",
       "          1.1609e+00,  6.2490e-01, -3.6863e-01, -3.2170e-01, -3.6580e-01,\n",
       "          4.1028e-01,  5.2209e-01,  5.8933e-01,  5.1508e-02,  1.0325e+00,\n",
       "         -6.2380e-01,  4.9736e-02, -5.5882e-01,  1.1945e+00, -1.0320e-01,\n",
       "          4.6326e-01, -7.8193e-01,  1.0335e+00,  5.7307e-02, -1.0779e+00,\n",
       "         -1.8786e-01,  5.2867e-02,  1.4848e-01, -1.2274e+00,  3.7243e-01,\n",
       "          1.0193e+00, -5.7713e-01, -1.2128e-01,  4.2395e-01, -2.8333e-01,\n",
       "         -1.2440e+00,  4.5804e-01,  3.3339e-01, -6.2717e-01,  6.8325e-01,\n",
       "          8.7844e-02,  4.4986e-01, -2.7255e-01,  5.1718e-02, -9.6954e-02,\n",
       "          4.2869e-01, -5.1876e-01,  7.0481e-01, -3.0843e-01,  1.3172e-01,\n",
       "         -6.4656e-02, -5.8071e-01,  1.1342e+00, -9.1504e-01,  5.9378e-01,\n",
       "         -1.9269e-01,  8.4756e-01,  9.7644e-01, -3.8886e-01,  4.4218e-01,\n",
       "          3.3841e-01, -5.1699e-01,  6.6908e-01,  2.7028e-01, -1.9406e-01,\n",
       "         -4.4060e-02,  7.6097e-02, -5.2202e-01, -6.3518e-01, -6.0413e-01,\n",
       "         -1.5235e-01,  1.9940e-02,  4.9916e-02, -1.7634e-01, -5.5294e-01,\n",
       "          2.4469e-01, -4.6409e-01, -8.6804e-01, -1.9443e-02,  4.4085e-03,\n",
       "         -8.5562e-02,  1.0591e+00,  9.4061e-01, -7.5945e-01, -9.1763e-02,\n",
       "          5.7631e-01, -1.3080e-01, -1.3504e-01,  8.4546e-01, -8.0415e-01,\n",
       "          4.5493e-01, -1.2751e-01,  1.0201e+00, -3.3451e-01,  2.3641e-01,\n",
       "         -6.5783e-02, -9.7805e-01, -1.1063e+00,  6.8125e-02,  5.9221e-02,\n",
       "         -4.5918e-01,  1.0936e+00, -1.9204e+00, -1.7917e-01,  1.5985e+00,\n",
       "         -1.1047e+00, -1.2077e+00,  7.2064e-01,  4.4163e-01,  8.7178e-02,\n",
       "         -5.2370e-01,  2.9561e-01, -1.9932e-01, -1.9933e-01, -1.0398e+00,\n",
       "         -9.2228e-01, -4.2394e-01, -1.0308e-01, -6.7119e-01,  2.1941e-01,\n",
       "         -5.0152e-01, -2.5628e-01,  9.5985e-01,  5.8777e-01, -1.1731e-02,\n",
       "          1.8428e-01, -1.2829e+00,  5.7508e-01, -1.2376e+00,  5.6131e-02,\n",
       "          9.0065e-01, -7.7213e-02, -6.5929e-01, -4.8660e-01,  1.6949e+00,\n",
       "         -6.4766e-02, -3.7448e-01,  1.0900e-01, -2.0859e-01,  2.0622e-01,\n",
       "          4.2757e-01,  5.2736e-01, -1.6155e-01,  5.0193e-01,  9.8693e-02,\n",
       "         -8.4288e-01, -7.1951e-01, -8.0396e-01,  2.6557e-01, -2.9157e-01,\n",
       "         -4.2161e-01,  3.6493e-01,  5.2247e-02, -1.0870e+00, -1.1359e+00,\n",
       "         -2.8983e-01, -2.6641e-01, -3.0282e-01, -9.7247e-02, -3.1193e-02,\n",
       "         -2.1661e-01, -3.3822e-01,  8.0896e-04, -4.2934e-01, -8.4703e-01,\n",
       "         -4.5067e-01,  1.2545e+00,  4.4002e-02,  8.9810e-01,  5.2982e-01,\n",
       "          9.9025e-03, -1.1341e-01,  3.6997e-01,  1.4271e-01, -2.8349e-01,\n",
       "          1.6910e-01,  5.1023e-01, -2.3283e-01, -3.3293e-01,  4.4425e-01,\n",
       "         -4.0868e-01, -3.8551e-01, -4.3105e-02,  7.0326e-01, -6.3375e-01,\n",
       "          2.3174e-01,  5.2791e-02,  3.2606e-01,  2.5682e-03, -3.5595e-01,\n",
       "         -4.6304e-01,  4.1712e-01,  2.4996e-01, -1.1906e+00, -2.0268e-01,\n",
       "         -6.8727e-01,  5.2194e-01,  6.2513e-01,  7.6073e-01,  2.7959e-01,\n",
       "          4.9104e-01, -6.2361e-01,  5.3061e-01, -1.1663e+00,  3.8195e-01,\n",
       "          4.8454e-02,  3.9087e-01,  3.9187e-01,  2.2919e-01, -1.0739e-01,\n",
       "          4.0507e-01,  2.0506e-01,  6.6460e-02,  1.2726e-01,  3.5287e-01,\n",
       "         -4.3007e-01, -5.3179e-01, -4.6893e-01,  4.5183e-01,  3.5667e-01,\n",
       "         -2.1813e-01, -1.4746e+00, -3.8654e-02, -5.5026e-01, -7.0673e-01,\n",
       "          4.5643e-01, -2.1455e-01, -9.9799e-01, -2.2702e-01, -2.6130e-01,\n",
       "          1.2449e+00,  3.3654e-01,  4.3913e-01, -7.0702e-02, -6.0870e-01,\n",
       "          3.5799e-01, -9.1622e-01,  1.1681e+00,  1.7650e-01,  7.6573e-02,\n",
       "          7.5811e-02,  1.0240e+00, -5.6189e-02,  6.9847e-01,  7.6325e-01,\n",
       "         -8.8126e-01,  4.6265e-01,  9.2210e-02,  2.6432e-01,  3.7891e-01,\n",
       "         -2.3489e-01,  1.9338e-01, -8.1108e-01,  3.0209e-01, -5.3560e-02,\n",
       "          1.8016e-01, -1.0686e+00, -3.4997e-01,  4.7199e-01,  5.2581e-01,\n",
       "         -8.7295e-02,  2.1356e-01,  2.9192e-01,  6.1634e-01, -9.8632e-01,\n",
       "          7.2117e-01,  1.6881e-01, -3.4396e-01, -8.5640e-02,  3.1718e-01,\n",
       "          1.3145e+00, -2.9744e-02,  6.4376e-01, -2.8307e-01,  3.0594e-01,\n",
       "          1.6880e-02, -2.8385e-02, -4.0107e-02, -1.1053e-02, -1.7188e-01,\n",
       "         -6.0942e-02,  3.7430e-01, -1.0131e-03,  1.2649e+00, -1.2661e-01,\n",
       "          1.4463e-01,  2.3123e-01,  3.9052e-01,  4.7551e-01,  3.6922e-01,\n",
       "         -2.2756e-01,  2.6511e-01, -5.7650e-01, -1.5270e-01, -8.4969e-01,\n",
       "          6.0404e-01, -1.0416e+00,  1.2639e-01,  8.0430e-02,  7.6046e-01,\n",
       "          5.6839e-01, -2.5595e-01,  1.1055e+00,  4.1597e-02, -6.3062e-01,\n",
       "         -2.0565e-01,  4.2533e-01,  6.8139e-01,  5.7517e-01, -2.7656e-01,\n",
       "          6.7681e-01, -6.2929e-01,  3.8977e-01,  9.0929e-02, -3.6747e-01,\n",
       "         -3.7598e-01,  1.9839e-01,  7.6227e-02, -3.9959e-01, -4.3450e-01,\n",
       "          5.0810e-01,  3.3016e-01,  9.3934e-01, -3.6420e-01, -4.4819e-02,\n",
       "         -2.2416e-01, -5.4732e-03, -1.0344e+00, -1.2244e-02, -3.7086e-01,\n",
       "          1.4640e-01, -7.0649e-01,  1.2186e+00, -1.2153e+00,  2.9681e-01,\n",
       "         -1.5064e+00, -3.7056e-01,  3.7817e-01,  8.9496e-01, -3.7711e-01,\n",
       "          1.1244e+00,  5.5395e-01, -9.4424e-02, -4.2505e-01, -3.0559e-01,\n",
       "          1.5443e-01, -7.3836e-02, -8.0571e-01,  1.6896e-01,  1.6799e-01,\n",
       "          3.8486e-01,  2.5077e-01,  5.8310e-01,  2.3383e-01, -2.1407e-01,\n",
       "          3.5485e-01, -1.7028e-01, -6.3472e-01, -1.0405e+00,  1.4612e-01,\n",
       "         -6.8937e-01,  1.4058e-01,  3.3279e-01, -7.8877e-01, -5.4970e-01,\n",
       "          2.3254e-01, -6.9347e-01,  7.0737e-02,  6.4487e-01,  1.6044e-02,\n",
       "          2.0178e-01, -4.0840e-01,  1.1530e-01, -1.1373e-02,  4.9034e-01,\n",
       "         -1.5098e-02, -4.0033e-01, -1.0154e+00, -2.0413e-01, -3.7169e-01,\n",
       "          2.7606e-01, -3.8299e-01, -6.1734e-01,  3.2347e-02, -4.5049e-01,\n",
       "         -6.3835e-01,  2.6976e-02, -3.8275e-01,  8.8007e-01,  2.6881e-01,\n",
       "          3.1463e-01,  2.5833e-01,  3.8731e-01, -5.2067e-01, -8.0411e-01,\n",
       "          2.4562e-01,  2.3688e-01,  1.9446e-01,  8.7052e-01, -6.4696e-02,\n",
       "         -8.5402e-01, -9.3957e-02, -1.6181e+00, -4.3831e-01,  3.6134e-01,\n",
       "         -7.0470e-02, -2.2463e-01, -2.5491e-01,  2.1521e-01, -1.1511e-01,\n",
       "          2.9751e-01, -6.4586e-02, -1.4485e-01,  4.9842e-01, -4.0209e-01,\n",
       "          4.1208e-01,  7.2566e-01, -9.4441e-02,  2.0319e-01, -6.3982e-01,\n",
       "          3.5116e-01,  2.9143e-01,  4.5694e-01, -9.2029e-01,  3.2455e-01,\n",
       "         -4.2873e-01, -3.3742e-01, -1.2537e+00,  3.1069e-01, -5.4949e-01,\n",
       "          8.6447e-01, -1.2808e-01, -2.7607e-01, -1.7612e-01,  3.3426e-01,\n",
       "          4.2745e-01, -5.6995e-01,  3.0312e-01,  5.4502e-01,  5.4927e-01,\n",
       "          5.8192e-01,  5.7464e-02, -8.3836e-01,  2.8787e-01, -5.7654e-01,\n",
       "         -2.5570e-01,  2.7647e-01,  4.2720e-01, -1.7824e-01,  8.1467e-02,\n",
       "          1.3164e-01,  3.1422e-01, -5.6658e-01, -3.4742e-01, -7.1744e-01,\n",
       "          3.5861e-02,  5.5098e-01, -2.4327e-01,  3.6379e-01, -2.0573e-01,\n",
       "          3.1384e-01, -2.1685e-01,  1.2692e-01, -4.3148e-01,  2.0168e-01,\n",
       "          1.6870e-01, -1.7744e-01,  4.6037e-01, -9.3660e-02, -6.8416e-01,\n",
       "          8.8972e-01, -2.5691e-01, -3.4674e-01,  5.7905e-01, -1.1934e+00,\n",
       "         -8.3628e-01, -1.0417e-01,  1.5082e+00,  2.3673e-01,  1.2553e-01,\n",
       "         -5.8326e-02,  5.0643e-01, -9.1203e-02,  1.8544e-02, -1.6355e-01,\n",
       "         -7.5146e-02, -5.6622e-01,  3.5891e-02,  5.4568e-02, -3.5137e-01,\n",
       "          1.9346e-01,  6.9995e-01,  5.0407e-01,  1.6141e-02,  2.8395e-01,\n",
       "         -1.0054e+00, -1.6014e-01, -1.5326e-01, -4.6076e-01,  2.5646e-01,\n",
       "          2.1092e-01,  4.5333e-01,  4.0446e-01, -1.1534e-01,  6.3081e-02,\n",
       "          2.4222e-01, -4.7722e-01,  5.6922e-01, -2.2030e-01, -5.9640e-01,\n",
       "          3.3780e-01,  1.2142e+00,  3.3001e-01,  3.5911e-01, -1.3442e-01,\n",
       "         -1.3253e-01,  4.0807e-01,  2.6249e-01,  3.6330e-01,  1.0035e+00,\n",
       "         -1.0540e+00, -3.1007e-01,  4.7330e-01,  6.4016e-01,  2.3191e-01,\n",
       "         -7.8581e-02,  1.2892e+00,  1.3783e+00,  4.1308e-01, -1.6698e-02,\n",
       "         -4.3543e-03,  3.4656e-01, -3.5282e-01, -7.5142e-01,  3.0562e-01,\n",
       "         -8.6074e-01, -4.7062e-01,  9.6007e-02,  4.5490e-01,  3.7043e-01,\n",
       "          9.1281e-01, -1.3376e+00, -2.0287e-01, -1.5355e-01, -4.6666e-01,\n",
       "          2.8115e-01, -6.4460e-03, -1.4640e+00,  8.4165e-01, -4.2126e-01,\n",
       "          6.7127e-01,  6.7656e-01, -4.7419e-02, -1.6642e-01, -7.8933e-03,\n",
       "         -9.4604e-01, -7.3082e-01, -6.2100e-01,  2.3823e-01, -9.2075e-02,\n",
       "          3.1756e-01,  1.1287e-01,  2.4301e-02,  1.0228e+00, -8.2903e-01,\n",
       "          9.4677e-01, -5.3734e-01,  4.6928e-01,  5.2468e-01,  2.0585e-01,\n",
       "          3.2971e-01, -5.3126e-01, -5.2426e-01,  1.8074e-01,  5.4569e-02,\n",
       "          1.2585e-01, -3.3821e-01, -2.3837e-01,  3.1705e-01,  1.5772e-01,\n",
       "         -2.2298e-01, -3.9903e-01,  8.1814e-01, -4.3042e-01, -1.0949e+00,\n",
       "         -4.7176e-03,  2.9526e-01,  1.7078e-01, -6.5543e-01,  4.9685e-01,\n",
       "         -4.0424e-01, -8.2567e-01, -4.0528e-01,  3.4557e-01, -8.2136e-01,\n",
       "         -4.1151e-01,  2.4558e-01, -7.4692e-01, -2.5524e-01, -4.6886e-01,\n",
       "         -9.5152e-01, -4.6141e-01,  5.2375e-01, -8.4813e-01,  3.3117e-02,\n",
       "         -2.9107e-01,  6.2394e-01, -3.8764e-01,  7.7019e-01, -4.0962e-01,\n",
       "         -3.0041e-01,  5.9089e-02,  3.1435e-01, -1.4739e-01, -7.5863e-02,\n",
       "         -7.7432e-01,  7.4017e-01, -1.0361e+00, -8.4468e-02, -6.7264e-01,\n",
       "         -4.7061e-01,  3.3461e-01, -4.5452e-01, -6.7156e-01, -4.3978e-01,\n",
       "         -8.9993e-01,  1.1584e+00, -9.1280e-01, -5.0706e-01,  8.9089e-01,\n",
       "          8.8204e-01, -1.8653e-01, -5.4693e-01, -9.0231e-01,  7.5464e-01,\n",
       "          2.5635e-01, -5.1758e-01,  7.4312e-01,  3.5031e-01,  7.3217e-01,\n",
       "          7.6462e-02, -9.3140e-02, -3.0219e-01,  9.0929e-01, -9.8953e-01,\n",
       "         -2.1549e-01, -6.9063e-02,  2.4409e-03,  6.3160e-01,  7.7572e-01,\n",
       "         -2.1661e-01, -1.8366e-01,  1.5737e-01,  3.1172e-01, -1.0572e-02,\n",
       "         -1.1528e-01,  7.0734e-01, -5.8508e-01, -1.1049e-01,  4.8251e-01,\n",
       "          3.4757e-01, -5.6440e-01,  8.2112e-01, -6.3577e-02, -2.2554e-01,\n",
       "         -6.6381e-01,  8.2322e-01,  1.9093e-01,  2.6485e-02, -5.1497e-01,\n",
       "         -7.3888e-02,  2.0224e-02, -2.3836e-01, -2.2997e-01,  5.1817e-01,\n",
       "          1.2159e+00, -2.9980e-01,  5.7447e-01, -1.1946e+00, -9.2907e-01,\n",
       "          1.5964e-01, -3.0438e-02,  4.0239e-02, -5.2135e-01,  2.0730e-01,\n",
       "         -4.8288e-01,  4.7793e-02, -3.0916e-01,  2.9829e-01, -1.0714e-01,\n",
       "          9.9107e-02,  5.3940e-01, -2.0047e-01, -4.1319e-01,  6.7519e-02,\n",
       "          4.6406e-01, -4.2428e-02, -9.4444e-01,  4.8697e-01,  5.2372e-01,\n",
       "         -2.5596e-01, -1.9811e-01,  5.0537e-01, -2.0716e-01,  3.2850e-01,\n",
       "         -6.1626e-01,  5.5327e-02, -1.1679e+00,  3.3481e-01,  6.8760e-01,\n",
       "          3.4203e-01, -6.6278e-01,  4.4150e-01,  2.1961e-01, -3.1441e-01,\n",
       "          2.5797e-01,  1.0047e-01,  1.0571e-01, -4.3527e-01,  2.8532e-01,\n",
       "         -3.7143e-01, -8.4182e-01,  8.6249e-01, -6.9354e-01, -7.6628e-02,\n",
       "         -1.1201e-01, -4.3603e-01, -1.8331e-01,  2.1101e-02,  1.0451e+00,\n",
       "         -3.3393e-02, -1.3146e+00,  3.0984e-01, -1.7800e-01,  2.5823e-01,\n",
       "         -2.5575e-01,  9.7707e-03,  2.6081e-01, -3.9144e-01,  5.6585e-01,\n",
       "          4.2876e-01,  1.8985e-01, -9.3828e-01, -6.2642e-01,  2.5072e-01,\n",
       "          7.5473e-02, -1.4985e-01,  1.2592e-01, -2.1142e-02,  1.0103e+00,\n",
       "          1.0445e-01, -7.5646e-01,  7.1895e-02, -1.6012e-01,  9.6911e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bb2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a075048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 60, 64])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(64, 64) \n",
    "input = torch.randn(1, 3, 60, 64)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a33b122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_config():\n",
    "    \"\"\"Base ViT config ViT\"\"\"\n",
    "    return dict(\n",
    "      dim=768,\n",
    "      ff_dim=3072,\n",
    "      num_heads=12,\n",
    "      num_layers=12,\n",
    "      attention_dropout_rate=0.0,\n",
    "      dropout_rate=0.1,\n",
    "      representation_size=768,\n",
    "      classifier='token'\n",
    "    )\n",
    "\n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = get_base_config()\n",
    "    config.update(dict(patches=(16, 16)))\n",
    "    return config\n",
    "\n",
    "def get_b32_config():\n",
    "    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n",
    "    config = get_b16_config()\n",
    "    config.update(dict(patches=(32, 32)))\n",
    "    return config\n",
    "\n",
    "def get_l16_config():\n",
    "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
    "    config = get_base_config()\n",
    "    config.update(dict(\n",
    "        patches=(16, 16),\n",
    "        dim=1024,\n",
    "        ff_dim=4096,\n",
    "        num_heads=16,\n",
    "        num_layers=24,\n",
    "        attention_dropout_rate=0.0,\n",
    "        dropout_rate=0.1,\n",
    "        representation_size=1024\n",
    "    ))\n",
    "    return config\n",
    "\n",
    "def get_l32_config():\n",
    "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
    "    config = get_l16_config()\n",
    "    config.update(dict(patches=(32, 32)))\n",
    "    return config\n",
    "\n",
    "def drop_head_variant(config):\n",
    "    config.update(dict(representation_size=None))\n",
    "    return config\n",
    "\n",
    "\n",
    "PRETRAINED_MODELS = {\n",
    "    'B_16': {\n",
    "      'config': get_b16_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16.pth\"\n",
    "    },\n",
    "    'B_32': {\n",
    "      'config': get_b32_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_32.pth\"\n",
    "    },\n",
    "    'L_16': {\n",
    "      'config': get_l16_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': None\n",
    "    },\n",
    "    'L_32': {\n",
    "      'config': get_l32_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/L_32.pth\"\n",
    "    },\n",
    "    'B_16_imagenet1k': {\n",
    "      'config': drop_head_variant(get_b16_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16_imagenet1k.pth\"\n",
    "    },\n",
    "    'B_32_imagenet1k': {\n",
    "      'config': drop_head_variant(get_b32_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_32_imagenet1k.pth\"\n",
    "    },\n",
    "    'L_16_imagenet1k': {\n",
    "      'config': drop_head_variant(get_l16_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/L_16_imagenet1k.pth\"\n",
    "    },\n",
    "    'L_32_imagenet1k': {\n",
    "      'config': drop_head_variant(get_l32_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/L_32_imagenet1k.pth\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb3feb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "\n",
    "def load_pretrained_weights(\n",
    "    model, \n",
    "    model_name=None, \n",
    "    weights_path=None, \n",
    "    load_first_conv=True, \n",
    "    load_fc=True, \n",
    "    load_repr_layer=False,\n",
    "    resize_positional_embedding=False,\n",
    "    verbose=True,\n",
    "    strict=True,\n",
    "):\n",
    "    \"\"\"Loads pretrained weights from weights path or download using url.\n",
    "    Args:\n",
    "        model (Module): Full model (a nn.Module)\n",
    "        model_name (str): Model name (e.g. B_16)\n",
    "        weights_path (None or str):\n",
    "            str: path to pretrained weights file on the local disk.\n",
    "            None: use pretrained weights downloaded from the Internet.\n",
    "        load_first_conv (bool): Whether to load patch embedding.\n",
    "        load_fc (bool): Whether to load pretrained weights for fc layer at the end of the model.\n",
    "        resize_positional_embedding=False,\n",
    "        verbose (bool): Whether to print on completion\n",
    "    \"\"\"\n",
    "    assert bool(model_name) ^ bool(weights_path), 'Expected exactly one of model_name or weights_path'\n",
    "    \n",
    "    # Load or download weights\n",
    "    if weights_path is None:\n",
    "        url = PRETRAINED_MODELS[model_name]['url']\n",
    "        if url:\n",
    "            state_dict = model_zoo.load_url(url)\n",
    "        else:\n",
    "            raise ValueError(f'Pretrained model for {model_name} has not yet been released')\n",
    "    else:\n",
    "        state_dict = torch.load(weights_path)\n",
    "\n",
    "    # Modifications to load partial state dict\n",
    "    expected_missing_keys = []\n",
    "    if not load_first_conv and 'patch_embedding.weight' in state_dict:\n",
    "        expected_missing_keys += ['patch_embedding.weight', 'patch_embedding.bias']\n",
    "    if not load_fc and 'fc.weight' in state_dict:\n",
    "        expected_missing_keys += ['fc.weight', 'fc.bias']\n",
    "    if not load_repr_layer and 'pre_logits.weight' in state_dict:\n",
    "        expected_missing_keys += ['pre_logits.weight', 'pre_logits.bias']\n",
    "    for key in expected_missing_keys:\n",
    "        state_dict.pop(key)\n",
    "\n",
    "    # Change size of positional embeddings\n",
    "    if resize_positional_embedding: \n",
    "        posemb = state_dict['positional_embedding.pos_embedding']\n",
    "        posemb_new = model.state_dict()['positional_embedding.pos_embedding']\n",
    "        state_dict['positional_embedding.pos_embedding'] = \\\n",
    "            resize_positional_embedding_(posemb=posemb, posemb_new=posemb_new, \n",
    "                has_class_token=hasattr(model, 'class_token'))\n",
    "        maybe_print('Resized positional embeddings from {} to {}'.format(\n",
    "                    posemb.shape, posemb_new.shape), verbose)\n",
    "\n",
    "    # Load state dict\n",
    "    ret = model.load_state_dict(state_dict, strict=False)\n",
    "    if strict:\n",
    "        assert set(ret.missing_keys) == set(expected_missing_keys), \\\n",
    "            'Missing keys when loading pretrained weights: {}'.format(ret.missing_keys)\n",
    "        assert not ret.unexpected_keys, \\\n",
    "            'Missing keys when loading pretrained weights: {}'.format(ret.unexpected_keys)\n",
    "        maybe_print('Loaded pretrained weights.', verbose)\n",
    "    else:\n",
    "        maybe_print('Missing keys when loading pretrained weights: {}'.format(ret.missing_keys), verbose)\n",
    "        maybe_print('Unexpected keys when loading pretrained weights: {}'.format(ret.unexpected_keys), verbose)\n",
    "        return ret\n",
    "\n",
    "\n",
    "def maybe_print(s: str, flag: bool):\n",
    "    if flag:\n",
    "        print(s)\n",
    "\n",
    "\n",
    "def as_tuple(x):\n",
    "    return x if isinstance(x, tuple) else (x, x)\n",
    "\n",
    "\n",
    "def resize_positional_embedding_(posemb, posemb_new, has_class_token=True):\n",
    "    \"\"\"Rescale the grid of position embeddings in a sensible manner\"\"\"\n",
    "    from scipy.ndimage import zoom\n",
    "\n",
    "    # Deal with class token\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if has_class_token:  # this means classifier == 'token'\n",
    "        posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "        ntok_new -= 1\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "\n",
    "    # Get old and new grid sizes\n",
    "    gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "    gs_new = int(np.sqrt(ntok_new))\n",
    "    posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "\n",
    "    # Rescale grid\n",
    "    zoom_factor = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "    posemb_grid = zoom(posemb_grid, zoom_factor, order=1)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "    posemb_grid = torch.from_numpy(posemb_grid)\n",
    "\n",
    "    # Deal with class token and return\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45a1308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16.pth\" to /Users/ratanimmidisetti/.cache/torch/hub/checkpoints/B_16.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f48c8106e694677a3afdc303253cd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/391M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Missing keys when loading pretrained weights: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.lk.weights', 'blocks.0.attn.lv.weights', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.lk.weights', 'blocks.1.attn.lv.weights', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.lk.weights', 'blocks.2.attn.lv.weights', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.lk.weights', 'blocks.3.attn.lv.weights', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.lk.weights', 'blocks.4.attn.lv.weights', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.lk.weights', 'blocks.5.attn.lv.weights', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.lk.weights', 'blocks.6.attn.lv.weights', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.lk.weights', 'blocks.7.attn.lv.weights', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.lk.weights', 'blocks.8.attn.lv.weights', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.lk.weights', 'blocks.9.attn.lv.weights', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.lk.weights', 'blocks.10.attn.lv.weights', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.lk.weights', 'blocks.11.attn.lv.weights', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'head.weight', 'head.bias']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-a16686e90152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m load_pretrained_weights(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B_16'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mload_first_conv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mload_fc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21843\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mload_repr_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-c112659ecc3d>\u001b[0m in \u001b[0;36mload_pretrained_weights\u001b[0;34m(model, model_name, weights_path, load_first_conv, load_fc, load_repr_layer, resize_positional_embedding, verbose, strict)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_missing_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;34m'Missing keys when loading pretrained weights: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munexpected_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Missing keys when loading pretrained weights: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.lk.weights', 'blocks.0.attn.lv.weights', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.lk.weights', 'blocks.1.attn.lv.weights', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.lk.weights', 'blocks.2.attn.lv.weights', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.lk.weights', 'blocks.3.attn.lv.weights', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.lk.weights', 'blocks.4.attn.lv.weights', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.lk.weights', 'blocks.5.attn.lv.weights', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.lk.weights', 'blocks.6.attn.lv.weights', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.lk.weights', 'blocks.7.attn.lv.weights', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.lk.weights', 'blocks.8.attn.lv.weights', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.lk.weights', 'blocks.9.attn.lv.weights', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.lk.weights', 'blocks.10.attn.lv.weights', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.lk.weights', 'blocks.11.attn.lv.weights', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'head.weight', 'head.bias']"
     ]
    }
   ],
   "source": [
    "load_pretrained_weights(\n",
    "                model, 'B_16', \n",
    "                load_first_conv=3,\n",
    "                load_fc=21843,\n",
    "                load_repr_layer=False,\n",
    "                resize_positional_embedding=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58b091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
