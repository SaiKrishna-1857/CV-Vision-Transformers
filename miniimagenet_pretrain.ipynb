{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9980961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from https://github.com/lukemelas/simple-bert\n",
    "\"\"\"\n",
    " \n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor \n",
    "from torch.nn import functional as F\n",
    "\n",
    "PEFT = 0\n",
    "\n",
    "def split_last(x, shape):\n",
    "    \"split the last dimension to given shape\"\n",
    "    shape = list(shape)\n",
    "    assert shape.count(-1) <= 1\n",
    "    if -1 in shape:\n",
    "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
    "    return x.view(*x.size()[:-1], *shape)\n",
    "\n",
    "\n",
    "def merge_last(x, n_dims):\n",
    "    \"merge the last n_dims to a dimension\"\n",
    "    s = x.size()\n",
    "    assert n_dims > 1 and n_dims < len(s)\n",
    "    return x.view(*s[:-n_dims], -1)\n",
    "\n",
    "class TrainableEltwiseLayer(nn.Module):\n",
    "    def __init__(self, n, h, w):\n",
    "        super(TrainableEltwiseLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.Tensor(1, n, h, w))  # define the trainable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assuming x is of size b-1-h-w\n",
    "        return x * self.weights  # element-wise multiplication\n",
    "\n",
    "\n",
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-Headed Dot Product Attention\"\"\"\n",
    "    def __init__(self, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.proj_q = nn.Linear(dim, dim)\n",
    "        self.proj_k = nn.Linear(dim, dim)\n",
    "        self.proj_v = nn.Linear(dim, dim)\n",
    "        \n",
    "        head_dim = dim // num_heads\n",
    "        #PEFT\n",
    "        if PEFT:\n",
    "            self.lk = TrainableEltwiseLayer(num_heads, head_dim, 26)\n",
    "            nn.init.ones_(self.lk.weights)\n",
    "            self.lv = TrainableEltwiseLayer(num_heads, 26, head_dim)\n",
    "            nn.init.ones_(self.lv.weights)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.n_heads = num_heads\n",
    "        self.scores = None # for visualization\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
    "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2) for x in [q, k, v])\n",
    "        \n",
    "        #PEFT\n",
    "        if PEFT:\n",
    "            k = self.lk(k.transpose(-2, -1))\n",
    "            v = self.lv(v)\n",
    "            scores = q @ k / np.sqrt(k.size(-1))\n",
    "        else:\n",
    "          # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
    "          scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :].float()\n",
    "            scores -= 10000.0 * (1.0 - mask)\n",
    "        scores = self.drop(F.softmax(scores, dim=-1))\n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
    "        h = (scores @ v).transpose(1, 2).contiguous()\n",
    "        # -merge-> (B, S, D)\n",
    "        h = merge_last(h, 2)\n",
    "        self.scores = scores\n",
    "        return h\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"FeedForward Neural Networks for each position\"\"\"\n",
    "    def __init__(self, dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, ff_dim)\n",
    "        #PEFT\n",
    "        if PEFT:\n",
    "            self.fcpeft = nn.Linear(ff_dim, ff_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(ff_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
    "        if PEFT:\n",
    "            return self.fc2(self.fcpeft(F.gelu(self.fc1(x))))\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block\"\"\"\n",
    "    def __init__(self, dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedSelfAttention(dim, num_heads, dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwff = PositionWiseFeedForward(dim, ff_dim)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        h = self.drop(self.proj(self.attn(self.norm1(x), mask)))\n",
    "        x = x + h\n",
    "        h = self.drop(self.pwff(self.norm2(x)))\n",
    "        x = x + h\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer with Self-Attentive Blocks\"\"\"\n",
    "    def __init__(self, num_layers, dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bb91bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model.py - Model and module class for ViT.\n",
    "   They are built to mirror those in the official Jax implementation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pytorch_pretrained_vit.utils import load_pretrained_weights, as_tuple\n",
    "from pytorch_pretrained_vit.configs import PRETRAINED_MODELS\n",
    "\n",
    "\n",
    "class PositionalEmbedding1D(nn.Module):\n",
    "    \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, seq_len, dim):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Input has shape `(batch_size, seq_len, emb_dim)`\"\"\"\n",
    "        return x + self.pos_embedding\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        name (str): Model name, e.g. 'B_16'\n",
    "        pretrained (bool): Load pretrained weights\n",
    "        in_channels (int): Number of channels in input data\n",
    "        num_classes (int): Number of classes, default 1000\n",
    "\n",
    "    References:\n",
    "        [1] https://openreview.net/forum?id=YicbFdNTTy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        name: Optional[str] = None, \n",
    "        pretrained: bool = False, \n",
    "        patches: int = 16,\n",
    "        dim: int = 768,\n",
    "        ff_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        num_layers: int = 12,\n",
    "        attention_dropout_rate: float = 0.0,\n",
    "        dropout_rate: float = 0.1,\n",
    "        representation_size: Optional[int] = None,\n",
    "        load_repr_layer: bool = False,\n",
    "        classifier: str = 'token',\n",
    "        positional_embedding: str = '1d',\n",
    "        in_channels: int = 3, \n",
    "        image_size: Optional[int] = None,\n",
    "        num_classes: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Configuration\n",
    "        if name is None:\n",
    "            check_msg = 'must specify name of pretrained model'\n",
    "            assert not pretrained, check_msg\n",
    "            #assert not resize_positional_embedding, check_msg\n",
    "            if num_classes is None:\n",
    "                num_classes = 1000\n",
    "            if image_size is None:\n",
    "                image_size = 384\n",
    "        else:  # load pretrained model\n",
    "            assert name in PRETRAINED_MODELS.keys(), \\\n",
    "                'name should be in: ' + ', '.join(PRETRAINED_MODELS.keys())\n",
    "            config = PRETRAINED_MODELS[name]['config']\n",
    "            patches = config['patches']\n",
    "            dim = config['dim']\n",
    "            ff_dim = config['ff_dim']\n",
    "            num_heads = config['num_heads']\n",
    "            num_layers = config['num_layers']\n",
    "            attention_dropout_rate = config['attention_dropout_rate']\n",
    "            dropout_rate = config['dropout_rate']\n",
    "            representation_size = config['representation_size']\n",
    "            classifier = config['classifier']\n",
    "            if image_size is None:\n",
    "                image_size = PRETRAINED_MODELS[name]['image_size']\n",
    "            if num_classes is None:\n",
    "                num_classes = PRETRAINED_MODELS[name]['num_classes']\n",
    "        self.image_size = image_size                \n",
    "\n",
    "        # Image and patch sizes\n",
    "        h, w = as_tuple(image_size)  # image sizes\n",
    "        fh, fw = as_tuple(patches)  # patch sizes\n",
    "        gh, gw = h // fh, w // fw  # number of patches\n",
    "        seq_len = gh * gw\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embedding = nn.Conv2d(in_channels, dim, kernel_size=(fh, fw), stride=(fh, fw))\n",
    "\n",
    "        # Class token\n",
    "        if classifier == 'token':\n",
    "            self.class_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "            seq_len += 1\n",
    "        \n",
    "        # Positional embedding\n",
    "        if positional_embedding.lower() == '1d':\n",
    "            self.positional_embedding = PositionalEmbedding1D(seq_len, dim)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = Transformer(num_layers=num_layers, dim=dim, num_heads=num_heads, \n",
    "                                       ff_dim=ff_dim, dropout=dropout_rate)\n",
    "        \n",
    "        # Representation layer\n",
    "        if representation_size and load_repr_layer:\n",
    "            self.pre_logits = nn.Linear(dim, representation_size)\n",
    "            pre_logits_size = representation_size\n",
    "        else:\n",
    "            pre_logits_size = dim\n",
    "\n",
    "        # Classifier head\n",
    "        self.norm = nn.LayerNorm(pre_logits_size, eps=1e-6)\n",
    "        self.fc = nn.Linear(pre_logits_size, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "        # Load pretrained model\n",
    "        if pretrained:\n",
    "            pretrained_num_channels = 3\n",
    "            pretrained_num_classes = PRETRAINED_MODELS[name]['num_classes']\n",
    "            pretrained_image_size = PRETRAINED_MODELS[name]['image_size']\n",
    "            load_pretrained_weights(\n",
    "                self, name, \n",
    "                load_first_conv=(in_channels == pretrained_num_channels),\n",
    "                load_fc=(num_classes == pretrained_num_classes),\n",
    "                load_repr_layer=load_repr_layer,\n",
    "                resize_positional_embedding=(image_size != pretrained_image_size),\n",
    "            )\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def init_weights(self):\n",
    "        def _init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # _trunc_normal(m.weight, std=0.02)  # from .initialization import _trunc_normal\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)  # nn.init.constant(m.bias, 0)\n",
    "        self.apply(_init)\n",
    "        nn.init.constant_(self.fc.weight, 0)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.normal_(self.positional_embedding.pos_embedding, std=0.02)  # _trunc_normal(self.positional_embedding.pos_embedding, std=0.02)\n",
    "        nn.init.constant_(self.class_token, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Breaks image into patches, applies transformer, applies MLP head.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): `b,c,fh,fw`\n",
    "        \"\"\"\n",
    "        b, c, fh, fw = x.shape\n",
    "        x = self.patch_embedding(x)  # b,d,gh,gw\n",
    "        x = x.flatten(2).transpose(1, 2)  # b,gh*gw,d\n",
    "        if hasattr(self, 'class_token'):\n",
    "            x = torch.cat((self.class_token.expand(b, -1, -1), x), dim=1)  # b,gh*gw+1,d\n",
    "        if hasattr(self, 'positional_embedding'): \n",
    "            x = self.positional_embedding(x)  # b,gh*gw+1,d \n",
    "        x = self.transformer(x)  # b,gh*gw+1,d\n",
    "        if hasattr(self, 'pre_logits'):\n",
    "            x = self.pre_logits(x)\n",
    "            x = torch.tanh(x)\n",
    "        if hasattr(self, 'fc'):\n",
    "            x = self.norm(x)[:, 0]  # b,d\n",
    "            x = self.fc(x)  # b,num_classes\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20528078",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT('B_16_imagenet1k', image_size=84, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1214784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47902c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyfsl.datasets import MiniImageNet\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 128\n",
    "n_workers = 4\n",
    "\n",
    "#train_set = MiniImageNet(root=\"data/mini_imagenet/\", transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((384, 384))]), split=\"train\", training=True)\n",
    "train_set = MiniImageNet(root=\"data/mini_imagenet/train/\", split=\"train\", training=True)\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dff800a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(train_set.get_labels()))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc575558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38400"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8c1ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBModel(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super().__init__()\n",
    "        self.vit = ViT('B_16_imagenet1k', image_size=84, pretrained=False)\n",
    "        self.linear1 = nn.Linear(1000,num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.vit(xb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed62ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CUBModel(num_classes = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74d2ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35400 3000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "torch.manual_seed(43)\n",
    "val_size = 3000\n",
    "train_size = len(train_set) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(train_set, [train_size, val_size])\n",
    "print(len(train_ds), len(val_ds))\n",
    "\n",
    "batch_size = 64\n",
    "n_workers = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "474dc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "def evaluate(model, val_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    acc_list = []\n",
    "    for batch in val_loader:\n",
    "        images, labels = batch\n",
    "        images = images.to(device=device, dtype=torch.float32)\n",
    "        labels = labels.to(device=device, dtype=torch.long)\n",
    "        out = model(images) \n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        acc_list.append(acc.item())\n",
    "    return sum(acc_list)/len(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 10\n",
      "Training epoch: [0/200]; step loss:[4.1614]; cumulative train loss:[4.1591]\n",
      "0.014295212765957447\n",
      "Time taken for the epoch 205.05043005943298\n",
      "Training epoch: [1/200]; step loss:[4.1566]; cumulative train loss:[4.1590]\n",
      "0.014295212765957447\n",
      "Time taken for the epoch 188.22474074363708\n",
      "Training epoch: [2/200]; step loss:[4.1614]; cumulative train loss:[4.1590]\n",
      "0.014295212765957447\n",
      "Time taken for the epoch 191.8555793762207\n",
      "Training epoch: [3/200]; step loss:[4.1617]; cumulative train loss:[4.1590]\n",
      "0.014295212765957447\n",
      "Time taken for the epoch 191.14019513130188\n",
      "Training epoch: [4/200]; step loss:[4.1608]; cumulative train loss:[4.1590]\n",
      "0.014342705184157858\n",
      "Time taken for the epoch 189.02425575256348\n",
      "Training epoch: [5/200]; step loss:[4.1529]; cumulative train loss:[4.1590]\n",
      "0.013962765957446808\n",
      "Time taken for the epoch 187.72257471084595\n"
     ]
    }
   ],
   "source": [
    "PEFT = 0\n",
    "model = CUBModel(num_classes = num_classes)\n",
    "if PEFT:\n",
    "    count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "            if 'lk' in name or 'lv' in name or 'fcpeft' in name or 'linear1' in name:\n",
    "                param.requires_grad = True\n",
    "                count += 1\n",
    "            else:\n",
    "                  param.requires_grad = False\n",
    "    print(count)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "# n_gpu = 0\n",
    "\n",
    "print(\"Number of GPUs: \"+str(n_gpu))\n",
    "\n",
    "train_losses = []\n",
    "n_epochs = 200\n",
    "\n",
    "#weights = torch.FloatTensor([0.05,0.12,0.23,0.6]).to(device)\n",
    "\n",
    "if n_gpu>=1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3,5,6])\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
    "\n",
    "results = []\n",
    "\n",
    "import time\n",
    "epoch_times = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    if n_gpu >=1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0,1,2,3,5,6])\n",
    "    #model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    total_loss = 0\n",
    "    batch_idx = 0\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch \n",
    "        images = images.to(device=device, dtype=torch.float32)\n",
    "        labels = labels.to(device=device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs.squeeze(),labels)\n",
    "        \n",
    "        if n_gpu>1:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        train_loss = train_loss + ((1/(batch_idx+1))*(loss.item()-train_loss))\n",
    "        total_loss = total_loss + loss.item()\n",
    "        batch_idx += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Training epoch: [{}/{}]; step loss:[{:.4f}]; cumulative train loss:[{:.4f}]'.format(epoch,n_epochs,loss.item(),train_loss))\n",
    "    temp_res = evaluate(model, val_loader)\n",
    "    results.append(temp_res)\n",
    "    print(temp_res)\n",
    "    scheduler.step()\n",
    "    end_time = time.time()\n",
    "    dur = end_time - start_time\n",
    "    print('Time taken for the epoch '+str(dur))\n",
    "    epoch_times.append(dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc452c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
